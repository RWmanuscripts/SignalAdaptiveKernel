#LyX 2.4 created this file. For more info see https://www.lyx.org/
\lyxformat 620
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children no
\language american
\language_package default
\inputencoding utf8
\fontencoding auto
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_roman_osf false
\font_sans_osf false
\font_typewriter_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement class
\float_alignment class
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_formatted_ref 0
\use_minted 0
\use_lineno 0
\index Index
\shortcut idx
\color #718c00
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tablestyle default
\tracking_changes false
\output_changes false
\change_bars false
\postpone_fragile_content true
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\docbook_table_output 0
\docbook_mathml_prefix 1
\end_header

\begin_body

\begin_layout Standard
The hyperparameters in the numerical experiments are fitted by the negative ML objective (Eq.
 4) for non-lazy-evaluation models,
 and by the data reduction method from Section 4.2 for lazy-evaluation models.
 Grid-based data such as grayscale images are used to facilitate easier visualization and interpretation for the continuity properties of the lazy-evaluation inference model and the ability to emphasize contours of the proposed warp map.
 Our work is meant to be a step towards regression modeling of non-grid-based observation data whilst carrying over some of these continuity and contour-preserving properties that are achievable by alternative methods for grid-based observations.
\end_layout

\begin_layout Standard
In Section 5.1,
 we visualize and interpret the proposed family of dimensional expansion kernels from Section 3.
 In Section 5.2,
 we visualize continuity-related issues for the patchwork kriging model,
 considered to be a state-of-the-art local GPR method.
 In section 5.3,
 we examine the lazy-evaluation combined with DE kernel on a set of grayscale images,
 to see there are no visually noticeable continuity-related issues and that contours are slightly better emphasized as discussed in Section 5.1.
 We use the conventional GPR inference (i.e.,
 non-lazy-evaluation) with the DE kernel in Sections 5.4 and 5.5 to examine the smoothing and the reduced oscillations of the predictive posterior for non-grid-based real-world rainfall and geology elevation datasets.
 We conclude this section with timing benchmarks of the methods used in our numerical experiments.
\end_layout

\begin_layout Section
Dimensional expansion kernel
\end_layout

\begin_layout Standard
We discussed the construction of two sets of warp samples:
 the HWRT-based samples from Eq.
 12 can only be made from grid-based data,
 and have issues near the boundary of the data,
 as shown in Fig 3.
 The BF-based samples from Eq.
 17 can be constructed from grid-based and non-grid-based data,
 and the numerical experiments are carried out with the BF-based warp map.
 The warp maps formed from these two sets of samples are shown in Fig.
 6 (smaller image size,
 make into one fig because emphasis is no longer on HRWT vs BF).
\end_layout

\begin_layout Standard
Fig markers show two markers on an image,
 which is taken to be the observations to a non-lazy-evaluation GPR model.
 The diamond marker is near an edge in the image,
 and the star marker is in an area with little edges.
 The last row of plots in Fig.
 are the show the difference between the stationary kernel and the dimensional kernel when one kernel input is taken to be a marker.
 The other input of the kernel is taken to be the neighboring locations near the marker.
 The presence of a non-zero warp map for the neighborhood near the diamond marker help shape the footprint of the kernel centered at marker to adjust to the contours of the warp map.
 The star marker is in a neighborhood where the warp map evaluates to near zero,
 so the dimensional expansion kernel centered at that marker is nearly the same as the canonical kernel.
\end_layout

\begin_layout Section
Patchwork kriging:
 continuity issues
\end_layout

\begin_layout Standard
A state-of-the-art local GPR model is Patchwork Kriging,
 where the input space is partitioned by some given spatial or k-d tree to form local models.
 It relies on a given set of points,
 called 
\emph on
pseudo observations
\emph default
,
 that must lie on the boundary of two local models.
 The pseudo observations must be given before query-time,
 and queries near the boundary of two local models are not in general discontinuous unless the query point is a pseudo observation.
 In practice,
 this meant query points on a boundary that have many nearby pseudo observations is likely to exhibit a smaller discontinuity step than the case where there is little nearby pseudo observations.
 We illustrate this issue in Fig pw_issues with a synthetic 2D dataset and a minimal setup of patchwork kriging as well as the conventional GPR method.
\end_layout

\begin_layout Section
Kodak Image Suite
\end_layout

\begin_layout Standard
The Kodak Image Suite dataset contains 24 images,
 and image up-conversion is a good visual method to inspect a regression method's performance,
 especially concerning continuity and the handling of edges in the input image.
 We take the grayscale version of the database images to be the 
\emph on
reference
\emph default
 images.
 The grid-based observation data was taken to be the reference down-sampled by two,
 i.e.,
 every other pixel is discarded.
 The query positions are taken to be the positions occupied by the pixels in the reference,
 i.e.,
 the up-conversion factor is two.
 This setup allows us to quantify image similarity between the reference and our results via the 
\emph on
Structural Similarity Index Measure
\emph default
 (SSIM) [ref].
 Since the observation data is too large for conventional GPR inference,
 we used the lazy-evaluation method.
 Unlike patchwork kriging,
 Theorem 4.x ensures our lazy-evaluation method should have valid continuity behavior everywhere on the input space.
 There are no visually noticeable discontinuous steps in the up-conversion results in Figs blah-blah.
\end_layout

\begin_layout Standard
Although the HRWT-based warp map can be used on image data,
 we use the BF-based warp map so we get good visualization of its behavior,
 before we use BF for non-grid data in the next experiment.
 Recall that our axis-search...
 (rest of paragraph)
\end_layout

\begin_layout Standard
The discarding of pixels...
 (entire rest of subsection)
\end_layout

\begin_layout Section
Rainfall measurements
\end_layout

\begin_layout Standard
Fig 7 shows the rainfall measurement for some Canadian weather stations near the city of Ottawa,
 Ontario on August 10,
 2023.
 The city of Ottawa experience significant rainfall but some nearby regions got little rain.
 This dataset provides a good demonstration for Runge and Gibbs-like oscillatory phenomenon for regression methods.
 The original data source was taken from the National Oceanic and Atmospheric Administration's (NOAA) Global Historical Climatology Network Daily dataset.
 This dataset was not large enough to require the use of lazy-evaluation GPR,
 so conventional GPR with DE kernel was used for this analysis.
 We did not experience the joint hyperparameters optimization issue from the Kodak experiments,
 so the two-stage optimization approach from Section 5.3 was not used here.
\end_layout

\begin_layout Standard
The hyperparameters for the DE kernel ...
 (rest of subsection)
\end_layout

\begin_layout Section
Lidar topographic measurements
\end_layout

\begin_layout Standard
Light detection and ranging (LiDAR) is a remote sensing technique that can be used to develop digital elevation models of land surface,
 which are used by power utilities,
 transportation infrastructure builders,
 and telecommunication companies to plan development projects.
 The LiDAR sensor might be on a satellite or aircraft,
 so the sampling pattern on the land does not always result in a grid.
 It is common to have regions with little LiDAR measurements because some wavelengths are absorbed by water or vegetation,
 and the measured range signal can appear noisy for some materials.
 For this reason,
 uncertainty information from regression models could potentially provide useful information to the users of LiDAR-based digital elevation models.
 We used a subset of the Missisquoi Watershed LiDAR dataset [] from OpenTopography to compare the result of the DE kernel with Natural Neighbors Sibson and inverse distance weighting,
 which are popular regression techniques in the applied topography community.
 The Julia packages NaturalNeighbours.jl and ScatteredInterpolations.jl were used for producing the Natural Neighbors Sibson and inverse distance weighting results,
 respectively.
 Lazy-evaluation GPR was not used in this experiment,
 as conventional GPR was tractable for the size of the dataset we used.
 The two-stage hyperparameter fitting approach from Sec.
 5.2 was used.
\end_layout

\begin_layout Standard
The left-most plot from Fig elevation_query illustrate the location of the LiDAR measurements,
 and the noisy scan-line sampling pattern that is probably due to the LiDAR satellite's trajectory are noticeable in the high elevation (i.e.,
 bright) portions of the queried results.
 The stationary kernel results removed the noisy appearance,
 but lost most of the details of the image.
 The inverse distance weighting and Sibson methods preserved the details,
 but have a lack of smoothing.
\end_layout

\begin_layout Standard
The predictive variance of the DE kernel's result have some larger values in regions that correspond to the noisy high elevation measurements.
 The Sibson and inverse distance weighting plots show a smooth region in the bottom right section,
 and the user would not be able to access whether the model is well-represented there or that there area lack of measurements in the region.
 Predictive posterior variance could be useful for users of elevation models so that subsequent remote sensing efforts with a different LiDAR wavelength can be allocated to measure the regions with high uncertainty of the regression model.
\end_layout

\begin_layout Section
Time benchmarks
\end_layout

\begin_layout Standard
\begin_inset Formula $U\subseteq X$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $N=\left|X\right|$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $O\left(N\right)$
\end_inset


\end_layout

\begin_layout Standard
We made a few trade-offs in the design of the proposed warp map-based DE kernels and lazy-evaluation inference works.
 The DE kernels require additional resources to evaluate the warp map for every query point,
 but the warp map evaluations for the training inputs can be pre-computed and stored in memory to speed up the computation of kernel matrices 
\begin_inset Formula $K_{X,X}$
\end_inset

 that involve subsets of the set of training inputs 
\begin_inset Formula $X$
\end_inset

.
 The computational complexity of the warp map extrapolation method adds to the overall time complexity of a GPR model that uses a DE kernel.
 We used the inverse distance weighting interpolation/extrapolation method to turn the warp samples into a warp map,
 which has a time-complexity of 
\begin_inset Formula $O\left(N\right)$
\end_inset

,
 where 
\begin_inset Formula $N$
\end_inset

 is the number of observations.
\end_layout

\begin_layout Standard
Computing the predictive mean can be significantly faster than the predictive variance for the conventional GPR predictive posterior when 
\begin_inset Formula $N$
\end_inset

 is large.
 This is because the 
\begin_inset Formula $\left(K_{X,X}+\sigma^{2}I\right)^{-1}y$
\end_inset

 portion of Eq blah can be pre-computed for any query location 
\begin_inset Formula $x^{*}$
\end_inset

,
 but a linear system that involves the query location needs to be solved in Eq.
 blah.
 Present day mainstream implementations of numerical linear system solvers have a time complexity of 
\begin_inset Formula $O\left(N^{3}\right)$
\end_inset

.
 The proposed lazy-evaluation approach replaces 
\begin_inset Formula $N$
\end_inset

 with the size of 
\begin_inset Formula $U$
\end_inset

,
 which is the set of observations in the query location's neighborhood of a given radius.
 Since 
\begin_inset Formula $U$
\end_inset

 depends on the query location,
 the vector 
\begin_inset Formula $\left(K_{U,U}+\sigma^{2}I\right)^{-1}y$
\end_inset

 cannot be pre-computed for any query location.
 This implies that when lazy-evaluation GPR will gradually lose its time-complexity advantage as the number of query points increase,
 and eventually be slower than conventional GPR.
 The size of 
\begin_inset Formula $U$
\end_inset

 also depends on the radius of the local neighborhood,
 and the number of training inputs in a query location's neighborhood,
 making this topic difficult to study.
 For this reason,
 our benchmarks on computing the predictive posterior are for only one query location.
 An in-depth complexity analysis as a function of all possible types of query point neighborhoods and query point numbers for the lazy-evaluation GPR is out of the scope of this work.
\end_layout

\begin_layout Standard
For all the benchmarks,
 we used a grayscale image that is small enough for conventional GPR and a query location that is within the image coordinate bounds.
 This is because for grid-based data,
 the size of 
\begin_inset Formula $U$
\end_inset

 should be similar for any interior query points away from the boundary of the grid.
 We cropped a region of an image from the Kodak Image Suite,
 and down-sampled it by different factors to get an image of size 
\begin_inset Formula $\left(95,105\right)$
\end_inset

,
 
\begin_inset Formula $\left(48,53\right)$
\end_inset

,
 and 
\begin_inset Formula $\left(32,35\right)$
\end_inset

 pixels.
 The lazy-evaluation GPR model used the data reduction strategy from Sec 4.2 to fit its hyperparameters.
 The tuning parameter 
\begin_inset Formula $M$
\end_inset

 from section 4.2 is an upper bound of the number of training entries to use,
 and Table hp_table showcase how different 
\begin_inset Formula $M$
\end_inset

 affect the time it took to fit the hyperparameters.
 A radius of 
\begin_inset Formula $6$
\end_inset

 pixels was used for the lazy-evaluation models in this table,
 similar to what was used in the image up-conversion experiment from Sec 5.2.
 We see the lazy-evaluation data reduction method offers significantly shorter times than the conventional GPR.
 For reference,
 
\begin_inset Formula $M$
\end_inset

 was set to 
\begin_inset Formula $50$
\end_inset

 for the experiments done in Sec 5.2.
\end_layout

\begin_layout Standard
Table con_query shows the times for computing the predictive posterior (both),
 just the predictive posterior mean (mean),
 and the pre-computation of the 
\begin_inset Formula $\left(K_{X,X}+\sigma^{2}I\right)^{-1}y$
\end_inset

 vector (setup) for the conventional GPR model.
 The analogous results for the lazy-evaluation GPR are shown in Table lazy_query,
 where the image size was set to 
\begin_inset Formula $\left(95,105\right)$
\end_inset

 because the size does not affect query speed.
 The neighborhood size is controlled by the local model radius,
 and this table shows it significantly affects the computation times.
\end_layout

\begin_layout Standard
In these tables,
 larger image sizes result in significantly longer computation times for both hyper parameter fitting and predictive posterior computation.
 The DE kernel was not significantly slower than the stationary kernel times for all tables.
 Some time benchmarks for other non-GPR regression or interpolation methods are shown in Table non_gpr.
 The bi-cubic 
\begin_inset Formula $B$
\end_inset

-spline method only works for grid-based data,
 but is very fast.
 The inverse distance weighting method can be used for non-grid based data,
 and requires no setup.
 The image with size 
\begin_inset Formula $\left(95,105\right)$
\end_inset

 was used for this table.
 The GPR methods are still quite slow than these alternative methods,
 but the predictive posterior variance information can be useful for applications such as spatial climate data or topological elevation modeling.
\end_layout

\begin_layout Section
DE
\end_layout

\begin_layout Standard

\lang english
The 
\emph on
dimensional expansion
\emph default
 (DE) kernel function that we use for this work has the following form for a 
\begin_inset Formula $D$
\end_inset

-dimensional input space:
 
\begin_inset Formula 
\begin{align}
k:\mathbb{R}^{D}\times\mathbb{R}^{D} & \rightarrow\mathbb{R},\label{eq:de_kernel_form}\\
\left(x,z\right) & \mapsto k_{\text{s}}\left(\begin{bmatrix}x\\
\phi\left(x\right)
\end{bmatrix},\begin{bmatrix}z\\
\phi\left(z\right)
\end{bmatrix}\right),\nonumber 
\end{align}

\end_inset

 where 
\begin_inset Formula $\phi:\mathcal{X}\rightarrow\mathbb{R}$
\end_inset

 is a 
\emph on
warping function
\emph default
,
 and 
\begin_inset Formula $k_{\text{s}}:\mathbb{R}^{D+1}\times\mathbb{R}^{D+1}\rightarrow\mathbb{R}$
\end_inset

 is a stationary kernel that we refer to as the 
\emph on
canonical kernel
\emph default
.
 A closely-related kernel function is the 
\emph on
warp kernel
\emph default
,
 and a Gaussian process prior that utilizes such a kernel is often referred to a 
\emph on
warp GP
\emph default
 model.
 It has the form 
\begin_inset Formula 
\begin{align}
k:\mathbb{R}^{D}\times\mathbb{R}^{D} & \rightarrow\mathbb{R},\label{eq:de_kernel_form-1}\\
\left(x,z\right) & \mapsto k_{\text{B}}\left(\psi\left(x\right),\psi\left(z\right)\right),\nonumber 
\end{align}

\end_inset

 for some generic map 
\begin_inset Formula $\psi:\mathbb{R}^{D}\rightarrow\mathbb{R}^{D}$
\end_inset

 and a 
\emph on
base kernel 
\begin_inset Formula $k_{\text{B}}$
\end_inset

.

\emph default
 The idea of introducing non-stationary behavior via the dimensional expansion or the warp kernel have been independently reported by many researchers in literature.
 Many warp kernel variants were reported in the survey 
\begin_inset CommandInset citation
LatexCommand cite
key "warpGP2001"
literal "false"

\end_inset

,
 and appeared in the recent 
\emph on
Deep kernel learning
\emph default
 (DKL) family of works,
 where 
\begin_inset Formula $\psi$
\end_inset

 is a neural network [].
 The first DE kernel that we are aware of appeared in 
\begin_inset CommandInset citation
LatexCommand cite
key "dimensional_expansion_kernel_2012"
literal "false"

\end_inset

,
 but was also reported as a 
\emph on
variably scaled kernel
\emph default
 (VSK) in the works 
\begin_inset CommandInset citation
LatexCommand cite
key "vs_kernel_interpolation_2015,vs_kernels_learning_2021,vs_kernels_persistent_homology_2022"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard

\lang english
The majority of the literature on warp GP or the DE kernel models fits the warp map jointly with the GPR hyperparameters according to some optimization problem.
 For example,
 many methods from the survey in [15] constrained the optimization over bijective maps for some spatial statistics application-specific requirements,
 while others in the survey and the DE work in [16] used a the thin-plate spline objective function to jointly learn the GP hyperparameters and the warp map or warp GP transformation map.
 The thin-plate spline objective function has roots from Reproducing Kernel Hilbert Space (RKHS) regularization theory,
 and its relationship to the GPR mean inference equations are discussed in [GPML].
 The DKL family of works focused on neural network tuning and optimization setup where the GP prior is simply the last layer of the neural network in a supervised learning framework.
 This implies that a separate group of datasets from the observed dataset is used to construct the warp map and hyperparameters of the GP prior,
 the fitted warp map adapts to the database instead of adapting to the observed signal.
 Therefore,
 the resultant predictive posterior are prone to hallucinations from the particular databases used in this supervised learning framework.
 Although this approach is sensible for many generative media or pattern classification applications,
 it is inappropriate for many other real-world applications where hallucinations are unacceptable.
 The VSK family of works [17-19] are dimensional expansion kernels with warp maps that are constructed from either specific analytical functions,
 or Naive Bayes classifier probabilities,
 as their emphasis was on GPR models for classification tasks.
 For regression tasks,
 they suggested to use least-squares to fit any generic regression model to the observed data,
 and use it as the warp map to the DE kernel.
 
\end_layout

\begin_layout Standard
The warp map construction methods from these existing works do not decouple the fitting of the GPR model parameters from the warp map construction,
 and the optimization problem is typically high-dimensional and challenging in practice [15,
 DKL pathology].
 It is also difficult to interpret how the jointly fitted warp map is actually affecting the predictive posterior when it is combined the DE kernel,
 so it is difficult to independently tune warp map-specific or GP prior-specific parameters without having to numerically solve the computationally-intensive model inference problem.
 In this work,
 we propose a decoupled approach to computing a warp map that works with both grid-based and non-grid-based datasets,
 and can be understood as an application of Fourier harmonic analysis and filter bank theory over grids and graphs.
 
\lang english
We focus on low-dimension regression problems in this paper,
 and use the marginal likelihood for selecting hyperparameters.
\end_layout

\end_body
\end_document
